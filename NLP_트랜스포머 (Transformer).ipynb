{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_트랜스포머 (Transformer).ipynb","provenance":[{"file_id":"1qBC_BPdmQgTWSd6n1B4d-pNAaATy_xVC","timestamp":1634197843253}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6PQSnsGeA2OH"},"source":["# 트랜스포머 (Transformer)\n","\n","* 참고: https://wikidocs.net/31379"]},{"cell_type":"markdown","metadata":{"id":"nbQ-h_XxBAiq"},"source":["* attention mechanism은 seq2seq의 입력 시퀀스 정보 손실을 보정해주기 위해 사용됨\n","* attention mechanism을 보정 목적이 아닌, 인코더와 디코더로 구성한 모델이 바로 트랜스포머\n","* 트랜스포머는 RNN을 사용하지 않고 인코더와 디코더를 설계하였으며, 성능도 RNN보다 우수함\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RDiFPIdUBBS2"},"source":["## 포지셔널 인코딩"]},{"cell_type":"markdown","metadata":{"id":"C0rWM6B8-g1b"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"rLqHf_4SEWoa"},"source":["* 기존의 RNN은 단어의 위치를 따라 순차적으로 입력받아 단어의 위치정보를 활용할 수 있었음\n","* 트랜스포머의 경우, RNN을 활용하지 않았기 때문에 단어의 위치정보를 다른 방식으로 줄 필요가 있음\n","* 이를 위해 **각 단어의 임베딩 벡터에 위치 정보들을 더하게 되는데** 이를 포지셔널 인코딩이라 함\n","* 보통 포지셔널 인코딩은 sin, cos을 이용하여 계산"]},{"cell_type":"code","metadata":{"id":"SiO5c_HIFBAk"},"source":["def positional_encoding(dim, sentence_length):\n","    encoded_vec = np.array([pos / np.power(10000, 2 * i / dim) for pos in range(sentence_length) for i in range(dim)])\n","    encoded_vec[::2] = np.sin(encoded_vec[::2])\n","    encoded_vec[1::2] = np.cos(encoded_vec[1::2])\n","    return tf.constant(encoede_vec.reshape([sentence_length, dim]), dtype=tf.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"099gUUxhAgy3"},"source":["## 레이어 정규화"]},{"cell_type":"markdown","metadata":{"id":"XCdips98yPuH"},"source":["*  레이어 정규화에서는 텐서의 마지막 차원에 대해 평균과 분산을 구하고, 이 값을 통해 값을 정규화함\n","*  해당 정규화를 각 층의 연결에 편리하게 적용하기 위해 함수화한 `sublayer_connection()`을 선언"]},{"cell_type":"code","metadata":{"id":"TSJjxF86Aeg3"},"source":["def layer_norm(inputs, eps=1e-6):\n","    feature_shape = input.get_shape()[-1:]\n","    mean = tf.keras.backed.mean(inputs, [-1], keepdims=True)\n","    std = tf.keras.backed.std(inputs, [-1], keepdims=True)\n","    bata = tf.Variable(tf.zeros(feature_shape), trainable=False)\n","    gamma = tf.Variable(tf.ones(feature_shape), trainable=False)\n","    return gamma * (inputs - mean) / (std + eps) + beta"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"km9ORxIun-MU"},"source":["def sublayer_connection(input, sublayer, dropout=0.2):\n","    outputs = layer_norm(inputs + tf.keras.laters.Dropout(dropout)(sublayer))\n","    return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ppb7IxJ3diMC"},"source":["## 어텐션"]},{"cell_type":"markdown","metadata":{"id":"1JaU6MHgy9V2"},"source":["\n","\n","*   트랜스포머 모델의 핵심이 되는 부분\n","*   트랜스포머에서는 multi-head attention과 self attention이라는 개념을 사용\n","  1.   multi-head attention\n","      * 디코더가 가지는 차원을 나누어 병렬로 어텐션을 진행\n","      *  마지막엔 병렬로 각 진행해 얻은 어텐션 헤드를 모두 연결\n","      * 이로 인해 다양한 시각에서 정보를 수집할 수 있는 효과를 얻음\n","  2.   self attention\n","      *   일반적인 어텐션의 경우, 특정 시점의 디코더 은닉상태와 모든 시점의 인코더 은닉상태를 활용\n","      *   이는 입력 문장과 다른 문장에 존재하는 단어간의 어텐션을 의미함\n","      *   반면 self attention은 은닉 상태를 동일하게 하여 어텐션을 진행\n","      *   이는 입력 문장 내 단어간의 어텐션을 의미함\n","\n","\n","\n","\n","*   트랜스포머 제안 논문에서는 scaled-dot product attention을 활용해 모델을 작성함\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kRyL0KDXi6ej"},"source":["### scaled-dot product attention 구현"]},{"cell_type":"markdown","metadata":{"id":"6HtmcgRR3Cr-"},"source":["* scaled-dot product attention은 앞서 학습한 dot product attention과 거의 유사함\n","* 단 attention을 진행할 때 어텐션 스코어를 계산할 때 내적 값을 정규화\n","* 트랜스포머에서는 정규화할 때 K 벡터(=디코더 셀의 은닉 상태)의 차원을 루트를 취한 값을 사용"]},{"cell_type":"code","metadata":{"id":"ALEMzi4fdiSQ"},"source":["def scaled_dot_product_attention(query, key, value, masked=False):\n","    key_dim_size = float(key.get_shape().as_list()[-1])\n","    key = tf.transpose(key, perm=[0, 2, 1])\n","\n","    outputs = tf.matmul(query, key) / tf.sqrt(key_dim_size)\n","\n","    if masked:\n","        diag_vals = tf.ones_like(outputs[0, :, :])\n","        tril = tf.linalg.LinearOperatorLwerTriangular(diag_vals).to_dense()\n","        masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(outputs)[0], 1, 1])\n","        paddings = tf.ones_like(masks)*(-2**30)\n","        outputs = tf.where(tf.equal(masks, 0), paddings, outputs)\n","\n","    attension_map = tf.nn.softmax(outputs)\n","    return tf.matmul(attention_map, value)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yr20BxvVi-8b"},"source":["### multi-head attention 구현"]},{"cell_type":"markdown","metadata":{"id":"Gb5qflUH14-H"},"source":["* multi-head attention의 구현 과정\n","  1. query, key, value에 해당하는 값을 받고, 해당 값에 해당하는 행렬 생성\n","  2. 생성된 행렬들을 heads에 해당하는 수만큼 분리\n","  3. 분리한 행렬들에 대해 각각 어텐션을 수행\n","  4. 각 어텐션 결과들을 연결해 최종 어텐션 결과 생성\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"ooc3FAdQi_Gz"},"source":["def multi_head_attention(query, key, value, num_units, heads, masked=False):\n","    query = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(query)\n","    key = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(key)\n","    value = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(value)\n","\n","    query = tf.concat(tf.split(query, heads, axis=-1), axis=0)\n","    key = tf.concat(tf.split(key, heads, axis=-1), axis=0)\n","    value = tf.concat(tf.split(value, heads, axis=-1), axis=0)\n","\n","    attension_map = scaled_dot_product_attention(query, key, value, masked)\n","    attn_outputs = tf.concat(tf.split(attention_map, heads, axis=0), axis=-1)\n","    attn_outputs = tf.keras.laters.Dense(num_units, activation=tf.nn.relu)(attn_outputs)\n","\n","    return attn_outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"78Zn5-fYITD4"},"source":["## 포지션-와이즈 피드 포워드 신경망"]},{"cell_type":"markdown","metadata":{"id":"-xxeG2xvo3ZN"},"source":["\n","\n","*   multi-head attention의 결과인 행렬을 입력받아 연산\n","*   일반적인 완전 연결 신경망(Dense layer)를 사용\n","*   position-wise FFNN은 인코더와 디코더에 모두 존재\n","\n"]},{"cell_type":"code","metadata":{"id":"0tSFd5OaITJ0"},"source":["def feed_dorward(inputs, num_units):\n","    feature_shape = input.get_shape()[-1]\n","    inner_layer = tf.keras.laters.Dense(num_units, activation=tf.nn.relu)(inputs)\n","    outputs = tf.keras.layers.Dense(feature_shape)(inner_layer)\n","    return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XuccViYgBK6v"},"source":["## 인코더\n"]},{"cell_type":"markdown","metadata":{"id":"tG3MH0n1JVLz"},"source":["* 인코더는 하나의 어텐션을 사용\n","  + encoder self-attention (multi-head self-attention과 동일)"]},{"cell_type":"code","metadata":{"id":"m5T0pzBoAnn3"},"source":["def encoder_module(inputs, model_dim, ffn_dim, heads):\n","    self_attn = sublayer_connection(inputs, multi_head_attention(inputs, inputs, inputs, model_dim, heads)\n","    outputs = sublayer_connection(self_attn, feed_forward(self_attn, ffn_dim))\n","    return outputs\n","\n","def encoder(inputs, model_dim, ffn_dim, heads, num_layers):\n","    outputs = inputs\n","    for i in range(num_layers):\n","        outputs = encoder_module(outputs, model_dim, ffn_dim, heads)\n","\n","    return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lcgHRcTEBQqg"},"source":["## 디코더"]},{"cell_type":"markdown","metadata":{"id":"cNj-6FLQwT4-"},"source":["* 디코더는 다음과 같은 구성의 반복으로 이루어짐\n","  1. masked decoder self-attention\n","  2. encoder-decoder attention\n","  3. position-wise FFNN\n","\n","* 디코더에서는 2종류의 어텐션을 사용\n","  1.   masked decoder self-attention\n","    *   디코더에서는 인코더와는 달리 순차적으로 결과를 만들어 내야하기 때문에 다른 어텐션 방법을 사용함\n","    *   디코더 예측 시점 이후의 위치에 attention을 할 수 없도록 masking 처리\n","    *   결국 예측 시점에서 예측은 미리 알고 있는 위치까지만의 결과에 의존\n","  2.   encoder-decoder attention\n","    *   앞서 설명한 multi-head attention과 동일\n","\n"]},{"cell_type":"code","metadata":{"id":"2B05wr7aARcT"},"source":["def decoder_module(inputs, encoder_outputs, model_dim, ffn_dim, heads):\n","    masked_self_attn = sublayer_connection(inputs,\n","                                           multi_head_attention(inputs, inputs, inputs,\n","                                                                model_dim, heads, masked=True))\n","    \n","    self_attn = sublayer_connection(masked_self_attn,\n","                                    multi_kead_attention(mask_self_attn,\n","                                                        encoder_outputs,\n","                                                        encoder_outputs,\n","                                                        model_dim, heads))\n","    outputs = sublayer_connection(self_attn, feed_forward(self_attn, ffn_dim))\n","    return outputs\n","\n","def decoder(inputs, encoder_outputs, model_dim, ffn_dim, heads, num_layers):\n","    outputs = inputs\n","    for i in range(num_layers):\n","        outputs = decoder_module(outputs, encoder_outputs, model_dim, ffn_dim, heads)\n","    \n","    return outputs                              \n","                                \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EtztlyUB1ERS"},"source":["## 트랜스포머를 활용한 챗봇"]},{"cell_type":"markdown","metadata":{"id":"6CGUIAzv6eWs"},"source":["### konlpy 라이브러리"]},{"cell_type":"markdown","metadata":{"id":"Ae0mHT49v5gy"},"source":["*    한글을 처리하기 위해 konlpy 라이브러리 설치"]},{"cell_type":"code","metadata":{"id":"U8yf75uG6hBW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634201974055,"user_tz":-540,"elapsed":6673,"user":{"displayName":"이지영","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2chENaxtL9YdQguKPKFbRgoMTO7ZXePMYvNlMJQ=s64","userId":"07841351895075120936"}},"outputId":"ac9d08dd-2229-4aeb-c844-8b9f9c4807f9"},"source":["!pip install konlpy"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n","\u001b[K     |████████████████████████████████| 19.4 MB 1.4 MB/s \n","\u001b[?25hCollecting colorama\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n","Collecting beautifulsoup4==4.6.0\n","  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 5.9 MB/s \n","\u001b[?25hCollecting JPype1>=0.7.0\n","  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n","\u001b[K     |████████████████████████████████| 448 kB 49.4 MB/s \n","\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n","Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n","  Attempting uninstall: beautifulsoup4\n","    Found existing installation: beautifulsoup4 4.6.3\n","    Uninstalling beautifulsoup4-4.6.3:\n","      Successfully uninstalled beautifulsoup4-4.6.3\n","Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"]}]},{"cell_type":"markdown","metadata":{"id":"rUMXvK5H1G9H"},"source":["### 데이터 준비"]},{"cell_type":"markdown","metadata":{"id":"miXrjR316mNb"},"source":["* 처리에 필요한 각종 변수 선언\n","* filters에 해당되는 문자를 걸러주는 정규 표현식 컴파일\n","\n"]},{"cell_type":"code","metadata":{"id":"SMjn5PfE1GZR"},"source":["import re\n","import tensorflow as tf\n","\n","filters = \"([~.,!?\\\"':;)(])\"\n","PAD = '<PADDING>'\n","STD = '<START>'\n","END = '<END>'\n","UNK = '<UNKNOWN>'\n","\n","PAD_INDEX = 0\n","STD_INDEX = 1\n","END_INDEX = 2\n","UNK_INDEX = 3\n","\n","MARKER = [PAD, STD, END, UNK]\n","CHANGE_FILTER = re.compile(filters)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xmRFuH2r6oNJ"},"source":["* 주소에서 데이터를 가져오는 `load_data()` 함수 선언\n","\n"]},{"cell_type":"code","metadata":{"id":"CmrmdXkePWYb"},"source":["from sklearn.model_selection import train_test_split\n","\n","def load_data(data_path):\n","    data_df = pd.read_csv(data_path, header=0)\n","    question, answer = list(data_df['Q']), list(data_df['A'])\n","    train_input, eval_input, train_label, eval_label = train_test_split(question, answer,\n","                                                                        test_size=0.33,\n","                                                                        random_state=13)\n","    return train_input, eval_input, train_label, eval_label"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vHuOJHPtPXqq"},"source":["* 처리에 필요한 단어 사전을 생성하는 `load_vocab()` 함수 선언"]},{"cell_type":"code","metadata":{"id":"QtQL-AP06oSa"},"source":["def load_vocabulary(data_path):\n","    data_df = pd.read_csv(data_path, encoding='utf-8')\n","    question, answer = list(data_df['Q']), list(data_df['A'])\n","    if tokenize_as_morph:\n","        question = prepro_like_morphlized(question)\n","        answer = prepro_like_morphlized(answer)\n","\n","    data = []\n","    data.extend(question)\n","    data.extend(answer)\n","    words = data_tokenizer(data)\n","    words = list(set(words))\n","    words[:0] = MAKKER\n","\n","    char2idx = {char:idx for idx, char in enumerate(words)}\n","    idx2char = {idx:char for idx, char in enumerate(words)}\n","    return char2idx, idx2char, len(char2idx)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5wYtpjv76r5q"},"source":["* 문자열 데이터를 학습에 사용될 수 있도록 변현하는 `prepro_like_morphlized()` 함수 선언\n","\n"]},{"cell_type":"code","metadata":{"id":"-bQ3FOva6tg6"},"source":["from konlpy.tag import Okt\n","\n","def prepro_like_morphlized(data):\n","    morph_analyzer = Okt()\n","    result_data = list()\n","    for seq in data:\n","        morphized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n","        result_data.append(morphized_seq)\n","    return result_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vhsVp4pWPTR3"},"source":["* 단어 사전을 만들기 위해 단어들을 분리하는 `data_tokenizer()` 함수 선언"]},{"cell_type":"code","metadata":{"id":"otLI_RUfPR_g"},"source":["def data_tokenizer(data):\n","    words = []\n","    for sentence in data:\n","        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n","        for word in sentence.split():\n","            words.append(word)\n","    return [word for word in words if word]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OkKPA-Mx6uaC"},"source":["* encoder의 입력을 구성하기 위한 함수 `enc_processing()` 선언\n","\n"]},{"cell_type":"code","metadata":{"id":"jK-yeSThPGsa"},"source":["def enc_processiog(value, dictionary):\n","    sequences_input_index = []\n","    sequences_length = []\n","\n","    if tokenize_as_morph:\n","        value = prepro_like_morphlized(value)\n","\n","    for sequence in value:\n","        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n","        sequence_index = []\n","        for word in sequence.split():\n","            if dictionary.get(word) is not None:\n","                sequence_index.extend([dictionary[word]])\n","            else:\n","                sequence_index.extend([dictionary[UNK]])\n","        if len(sequence_index) > max_len:\n","            sequence_index = sequence_index[:max_len]\n","        sequences_length.append(len(sequence_index))\n","        sequence_index += (max_len - len(sequence_index)) * [dictionary[PAD]]\n","        sequences_input_index.append(sequence_index)\n","    return np.asarray(sequences_input_index), sequences_length"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d4mM57_FPIg7"},"source":["* decoder의 입력을 구성하기 위한 함수 `dec_input_processing()` 선언"]},{"cell_type":"code","metadata":{"id":"cX_NpcTq6vw6"},"source":["def dec_output_processiog(value, dictionary):\n","    sequences_output_index = []\n","    sequences_length = []\n","\n","    if tokenize_as_morph:\n","        value = prepro_like_morphlized(value)\n","\n","    for sequence in value:\n","        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n","        sequence_index = []\n","        sequence_index = [dictionary[STD]] + [dictionary[word] for word in sequence.split()]\n","        if len(sequence_index) > max_len:\n","            sequence_index = sequence_index[:max_len]\n","        sequences_length.append(len(sequence_index))\n","        sequence_index += (max_len - len(sequence_index)) * [dictionary[PAD]]\n","        sequences_output_index.append(sequence_index)\n","    return np.asarray(sequences_output_index), sequences_length"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"otsTEt4FPLJX"},"source":["* decoder의 출력을 구성하기 위한 함수 `dec_target_processing()` 선언"]},{"cell_type":"code","metadata":{"id":"eeP0PWHEPMma"},"source":["def dec_target_processiog(value, dictionary):\n","    sequences_target_index = []\n","    if tokenize_as_morph:\n","        value = prepro_like_morphlized(value)\n","\n","    for sequence in value:\n","        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n","        sequence_index = []\n","        sequence_index = [dictionary[word] for word in sequence.split()]\n","        if len(sequence_index) >= max_len:\n","            sequence_index = sequence_index[:max_len -1] + [dictionary[END]]\n","        else:\n","            sequence_index += [dictionary[END]]\n","        sequence_index += (max_len - len(sequence_index)) * [dictionary[PAD]]\n","        sequences_target_index.append(sequence_index)\n","    return np.asarray(sequences_target_index), sequences_length"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tb9vVUng6xDq"},"source":["* 모델에 데이터를 효율적으로 투입하도록 `train_input_fn()`, `eval_input_fn()` 함수 선언\n","* `rearrange()`는 dataset 객체가 데이터를 어떻게 변형시킬지 정의해둔 함수\n","* dataset.map은 rearrange 함수를 기반으로 데이터를 변형\n","\n"]},{"cell_type":"code","metadata":{"id":"uAlKV4xF62Uf"},"source":["def train_input_fn(train_input_enc, train_output_enc, train_target_dec, batch_size):\n","    dataset = tf.compat.v1.data.Dataset.from_tensor_slices((train_input_enc, train_output_enc, train_target_dec))\n","    dataset = dataset.shuffle(buffer_size = len(train_input_enc))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(rearrange)\n","    dataset = datset.repeat()\n","    iterator = dataset.make_one_shot_interator()\n","    return interator.get_next()\n","\n","def eval_input_fn(eval_input_enc, eval_output_enc, eval_target_dec, batch_size):\n","    dataset = tf.compat.v1.data.Dataset.from_tensor_slices((eval_input_enc, eval_output_enc, eval_target_dec))\n","    dataset = dataset.shuffle(buffer_size = len(eval_input_enc))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(rearrange)\n","    dataset = datset.repeat()\n","    iterator = dataset.make_one_shot_interator()\n","    return interator.get_next()\n","\n","def rearrange(input, output, target):\n","    features = {'input': input, 'output':output}\n","    return features, target\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"is-GhUDN62xC"},"source":["* 모델의 예측은 배열로 생성되기 때문에 이를 확인하기 위해선 문자열로 변환이 필요\n","* 예측을 문자열로 변환해주는 `pred2string()` 함수 선언\n"]},{"cell_type":"code","metadata":{"id":"jCfwWXhb64Cc"},"source":["def pred2string(value, dictionary):\n","    sentence_string = []\n","    is_finished = False\n","    for v in value:\n","        sentence_string = [dictionary[index] for index in v['index']]\n","\n","        answer = \"\"\n","        for word in sentence_string:\n","            if word == END:\n","                if_finished = True\n","                break\n","\n","            if word != PAD and word != END:\n","                answer += word\n","                answer += \" \"\n","\n","        return answer, is_finished"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hwp9Nnwz7UoG"},"source":["* 챗봇 데이터 URL: https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv\n","* 데이터 주소에서 데이터를 읽어들여 단어 사전과 사용 데이터 구성"]},{"cell_type":"code","metadata":{"id":"-T536MdU7Taq","colab":{"base_uri":"https://localhost:8080/","height":218},"executionInfo":{"status":"error","timestamp":1634477068468,"user_tz":-540,"elapsed":601,"user":{"displayName":"이지영","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj2chENaxtL9YdQguKPKFbRgoMTO7ZXePMYvNlMJQ=s64","userId":"07841351895075120936"}},"outputId":"5a3e0834-ab6e-4163-acd2-f6130f4fc852"},"source":["import pandas as pd\n","\n","tokenize_as_morph = True\n","\n","data_path = 'https://github.com/songys/Chatbot_data/blob/f8f1c206fbf6d7fe25a4b99f60aee2448b432de6/ChatbotData.csv'\n","\n","char2idx, idx2char, len_vocab = load_vocabulary(data_path)\n","train_input, train_label, eval_input, eval_label = load_data(data_path)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-0eee5d4b06b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://github.com/songys/Chatbot_data/blob/f8f1c206fbf6d7fe25a4b99f60aee2448b432de6/ChatbotData.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mchar2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'load_vocabulary' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"7cVd7AOKinqn"},"source":["### 모델 구성"]},{"cell_type":"code","metadata":{"id":"g9sAg90BVokC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hqLJ0a6r49yi"},"source":["* 앞서 작성한 트랜스포머 모델을 결합해 학습에 사용할 모델을 구성함"]},{"cell_type":"code","metadata":{"id":"CNeeXoZginvj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H7PrLEWE1JCs"},"source":["### 모델 학습"]},{"cell_type":"markdown","metadata":{"id":"Gy_Opm_A7DKC"},"source":["*   필요한 각종 인자들을 설정\n","*   인자에 따라 학습 결과가 달라질 수 있기 때문에 세심한 조정이 필요\n"]},{"cell_type":"code","metadata":{"id":"CKGYuqmH6_kj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aaXalEy57ODq"},"source":["*   앞서 선언한 processing 함수로 데이터를 모델에 투입할 수 있도록 가공\n","*   평가 데이터에도 동일하게 가공"]},{"cell_type":"code","metadata":{"id":"NWlgWWIq1KSh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qZGgZzWs7Mr7"},"source":["* 앞서 선언한 함수를 통해 모델을 선언하고 학습\n","* `tf.estimator`를 사용해 간편하게 학습 모듈 구성\n"]},{"cell_type":"code","metadata":{"id":"B9vjc3Ck7F4J"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wl_pwUiw7INZ"},"source":["* 학습한 모델을 사용해 챗봇을 사용\n","* 예측 결과를 문자열로 변환할 때는 앞서 선언한 `pred2string()` 함수를 이용\n","* 입력에 대한 응답이 생성되는 것을 확인할 수 있음\n"]},{"cell_type":"code","metadata":{"id":"COO-0PcS7Hy5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MNcrVf2z1LSM"},"source":["### 예측"]},{"cell_type":"markdown","metadata":{"id":"R5lY9DrW8eSK"},"source":["* 학습한 모델을 사용해 챗봇을 사용\n","* 예측 결과를 문자열로 변환할 때는 앞서 선언한 `pred2string()` 함수를 이용\n","* 입력에 대한 응답이 생성되는 것을 확인할 수 있음\n"]},{"cell_type":"code","metadata":{"id":"N9IQaBx4Qw8J"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IjHZKvJ31MAU"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_mjRZwyLQ_gP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T7AJCsXRTqJx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_M8mfoUfeAWQ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5mrdGRaem6v"},"source":[""],"execution_count":null,"outputs":[]}]}